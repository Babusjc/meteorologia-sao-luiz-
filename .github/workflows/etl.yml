name: Data Pipeline

on:
  schedule:
    - cron: '0 0 * * *'  # Executa diariamente à meia-noite
  workflow_dispatch:

# Adicionar permissões para permitir push
permissions:
  contents: write

jobs:
  download-and-process:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
      with:
        fetch-depth: 0  # Importante para o git push funcionar

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Download data (FORÇADO)
      run: |
        mkdir -p data/raw
        echo "Forçando download de todos os arquivos..."
        python scripts/download_data.py --force

    - name: Verificar arquivos baixados
      run: |
        echo "Arquivos na pasta data/raw:"
        ls -la data/raw/
        echo ""
        echo "Tamanho dos arquivos:"
        du -h data/raw/*.csv | head -10

    - name: Verificar conteúdo dos arquivos CSV
      run: |
        echo "Conteúdo do primeiro arquivo CSV:"
        head -n 3 data/raw/INMET_SE_SP_A740_SAO_LUIZ_DO_PARAITINGA_2024.csv
        echo ""
        echo "Estrutura dos arquivos CSV (primeira linha):"
        for file in data/raw/*.csv; do
          echo "=== $file ==="
          head -n 1 "$file"
          echo ""
        done | head -5

    - name: Verificar encoding e separadores
      run: |
        echo "Verificando encoding e separadores dos arquivos:"
        for file in data/raw/*.csv; do
          echo "=== $file ==="
          file -i "$file"
          echo "Primeiras 2 linhas:"
          head -n 2 "$file"
          echo "Número de campos na primeira linha:"
          head -n 1 "$file" | tr '\t' ' ' | wc -w
          echo ""
        done | head -20

    - name: Create train_model.py if not exists
      run: |
        if [ ! -f "scripts/train_model.py" ]; then
          echo "Criando scripts/train_model.py..."
          cat > scripts/train_model.py << 'EOF'
        import pandas as pd
        import numpy as np
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import classification_report, accuracy_score
        from sklearn.preprocessing import StandardScaler
        from sklearn.pipeline import make_pipeline
        import joblib
        from pathlib import Path
        import logging

        # Configurar logging
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

        def train_precipitation_model():
            # Carregar dados processados
            data_path = Path("data/processed/processed_weather_data.parquet")
            if not data_path.exists():
                logging.error("Dados processados não encontrados")
                return
            
            try:
                df = pd.read_parquet(data_path)
                logging.info(f"Dados carregados com {len(df)} registros")
            except Exception as e:
                logging.error(f"Erro ao carregar dados: {str(e)}")
                return
            
            # Criar coluna datetime combinada para análise temporal
            if 'data' in df.columns and 'hora' in df.columns:
                df['datetime'] = pd.to_datetime(df['data'].astype(str) + ' ' + df['hora'].astype(str))
                df['hour'] = df['datetime'].dt.hour
                df['weekday'] = df['datetime'].dt.dayofweek
                df['month'] = df['datetime'].dt.month
            
            # Preparar dados para modelagem
            if "precipitacao_total" not in df.columns:
                logging.error("Dados de precipitação ausentes")
                return
            
            # Criar variável alvo (próxima hora)
            df["target"] = df["precipitacao_total"].shift(-1).fillna(0)
            
            # Classificar chuva em categorias
            conditions = [
                (df["target"] == 0),
                (df["target"] > 0) & (df["target"] <= 2.5),
                (df["target"] > 2.5)
            ]
            choices = [0, 1, 2]  # 0: Sem chuva, 1: Chuva leve, 2: Chuva forte
            df["rain_class"] = np.select(conditions, choices, default=0)
            
            # Features
            features = [
                "temperatura_ar", "umidade_relativa", "pressao_atm_estacao",
                "radiacao_global", "temperatura_max", "temperatura_min",
                "hour", "weekday", "month"
            ]
            
            # Filtrar apenas colunas disponíveis
            available_features = [f for f in features if f in df.columns]
            
            # Filtrar dados completos
            df = df.dropna(subset=available_features + ["rain_class"])
            
            if df.empty:
                logging.error("Nenhum dado disponível para treinamento")
                return
            
            X = df[available_features]
            y = df["rain_class"]
            
            # Verificar se temos dados suficientes para treinamento
            if len(X) < 100:
                logging.error(f"Dados insuficientes para treinamento: apenas {len(X)} amostras")
                return
            
            # Dividir dados
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
            
            logging.info(f"Treinando modelo com {len(X_train)} amostras")
            
            # Criar e treinar modelo
            try:
                model = make_pipeline(
                    StandardScaler(),
                    RandomForestClassifier(
                        n_estimators=100,
                        max_depth=10,
                        class_weight="balanced",
                        random_state=42
                    )
                )
                
                model.fit(X_train, y_train)
                logging.info("Modelo treinado com sucesso")
                
                # Avaliar
                y_pred = model.predict(X_test)
                accuracy = accuracy_score(y_test, y_pred)
                logging.info(f"Acurácia: {accuracy:.2f}")
                logging.info("\nRelatório de Classificação:\n" + classification_report(y_test, y_pred))
                
                # Salvar modelo
                model_dir = Path("models")
                model_dir.mkdir(exist_ok=True)
                joblib.dump(model, model_dir / "precipitation_model.pkl")
                logging.info("Modelo salvo com sucesso")
                
            except Exception as e:
                logging.error(f"Erro ao treinar modelo: {str(e)}")

        if __name__ == "__main__":
            train_precipitation_model()
        EOF
        fi

    - name: RUN ETL and ML Training
      env:
        NEON_DB_URL: ${{ secrets.NEON_DB_URL }}
      run: |
        mkdir -p data/processed
        mkdir -p models
        
        echo "Executando ETL com logging detalhado..."
        python scripts/etl_local.py
        
        # Verificar se o ETL criou os dados processados
        if [ -f "data/processed/processed_weather_data.parquet" ]; then
          echo "Dados processados encontrados, treinando modelo..."
          python scripts/train_model.py
          
          # Verificar se os dados foram inseridos no banco
          echo "Verificando dados no banco Neon..."
          python -c "
          import pandas as pd
          from scripts.database import get_data_from_db
          try:
              df = get_data_from_db('SELECT COUNT(*) as total FROM meteo_data')
              print(f'Total de registros no banco: {df[\"total\"][0]}')
              
              # Verificar dados mais recentes
              recent_data = get_data_from_db('SELECT * FROM meteo_data ORDER BY data DESC, hora DESC LIMIT 5')
              print('Dados mais recentes no banco:')
              print(recent_data)
          except Exception as e:
              print(f'Erro ao consultar banco: {e}')
          "
        else
          echo "AVISO: Nenhum dado processado disponível"
          echo "Conteúdo da pasta data/raw:"
          ls -la data/raw/
          echo "Primeiras linhas de um arquivo CSV:"
          head -n 5 data/raw/INMET_SE_SP_A740_SAO_LUIZ_DO_PARAITINGA_2024.csv
          echo "Verificando problemas de encoding:"
          file -i data/raw/INMET_SE_SP_A740_SAO_LUIZ_DO_PARAITINGA_2024.csv
        fi

    - name: Commit processed data
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add data/processed/*
        git add models/*
        git add scripts/train_model.py  # Adicionar o script se foi criado
        git commit -m "Update processed data and models [$(date +%Y-%m-%d)]" || echo "No changes to commit"
        git push origin main

